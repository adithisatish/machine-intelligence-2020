{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mode\n",
    "from Neural_Net import boxplots, dataCleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_temp = pd.read_csv(\"LBW_Dataset.csv\")\n",
    "dataset_temp = dataCleaning(dataset_temp)\n",
    "\n",
    "# features is a dataframe containing all the different features (attributes) of our dataset\n",
    "features = dataset_temp[['Community','Age','Weight','Delivery phase','HB','IFA','BP','Education','Residence']] # Copy isn't needed lol\n",
    "\n",
    "# labels is a dataframe containing the corresponding results that we try to predit using the NN\n",
    "labels = dataset_temp['Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42) #we use random.seed so that we can get the same random values whenever executed\n",
    "#weights = np.random.rand(9,96) #9 since we have 9 attributes\n",
    "#bias = np.random.rand(96) #randomly choose any bias value\n",
    "#learning_rate = 0.05 #usually 0.05 is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 9)\n",
      "(96,)\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "91    1\n",
      "92    1\n",
      "93    1\n",
      "94    1\n",
      "95    1\n",
      "Name: Result, Length: 96, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def MSE(y_hat, y):\n",
    "    return np.mean(np.square(y-y_hat))\n",
    "\n",
    "def layerSize(x, y):\n",
    "    x_size = x.shape[0]\n",
    "    y_size = y.shape[0]\n",
    "    h_size = 4 #hidden layer\n",
    "    return(x_size, h_size, y_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for epoch in range(100): #x times the model is trained. we can choose different value based on our error.\\n    inputs = features\\n    xDOTw = np.dot(features, weights) + bias # dot product of xi.wi\\n    #print(xDOTw)\\n    netk = sigmoid(xDOTw)\\n    labels = np.array(labels)\\n    #print(type(netk))\\n    #print(type(labels))\\n    \\n    #back propagation begins\\n    error = MSE(netk,labels)\\n    print(\"Error = \", error)\\n    \\n    # dE/dw = dE/dOk * dOk/dnetk * dnetk/dw\\n    dE_dOk = error # dE/dOk\\n    dok_dnetk = derivative_sigmoid(netk) #dOk/dnetk\\n    dz_dw = dE_dOk * dok_dnetk # the derivative with respect to any weight is simply the corresponding input\\n\\n    inputs = features.T #transpose of the matrix\\n    weights -= learning_rate * np.dot(inputs, dz_dw)\\n\\n    #print(dz_dw)\\n    for num in dz_dw:\\n        bias -= learning_rate * num\\n    '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for epoch in range(100): #x times the model is trained. we can choose different value based on our error.\n",
    "    inputs = features\n",
    "    xDOTw = np.dot(features, weights) + bias # dot product of xi.wi\n",
    "    #print(xDOTw)\n",
    "    netk = sigmoid(xDOTw)\n",
    "    labels = np.array(labels)\n",
    "    #print(type(netk))\n",
    "    #print(type(labels))\n",
    "    \n",
    "    #back propagation begins\n",
    "    error = MSE(netk,labels)\n",
    "    print(\"Error = \", error)\n",
    "    \n",
    "    # dE/dw = dE/dOk * dOk/dnetk * dnetk/dw\n",
    "    dE_dOk = error # dE/dOk\n",
    "    dok_dnetk = derivative_sigmoid(netk) #dOk/dnetk\n",
    "    dz_dw = dE_dOk * dok_dnetk # the derivative with respect to any weight is simply the corresponding input\n",
    "\n",
    "    inputs = features.T #transpose of the matrix\n",
    "    weights -= learning_rate * np.dot(inputs, dz_dw)\n",
    "\n",
    "    #print(dz_dw)\n",
    "    for num in dz_dw:\n",
    "        bias -= learning_rate * num\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
