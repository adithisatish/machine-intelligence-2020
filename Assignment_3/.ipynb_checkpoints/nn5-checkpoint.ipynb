{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mode\n",
    "from Neural_Net import boxplots, dataCleaning\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_temp = pd.read_csv(\"LBW_Dataset.csv\")\n",
    "dataset_temp = dataCleaning(dataset_temp)\n",
    "# dataset_temp[\"Result\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a dataframe containing all the different features (attributes) of our dataset\n",
    "features = dataset_temp[['Community','Age','Weight','Delivery phase','HB','IFA','BP','Education','Residence']] # Copy isn't needed lol\n",
    "# labels is a dataframe containing the corresponding results that we try to predit using the NN\n",
    "labels = dataset_temp[['Result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Seed the random number generator\n",
    "        np.random.seed(1)\n",
    "        self.lr = 0.01\n",
    "        # Set the weights -> 10 hidden layer neurons \n",
    "        self.input_hidden_weights = np.random.randn(9, 20)*0.03\n",
    "        self.output_hidden_weights = np.random.randn(20,1)*0.03\n",
    "        # Setting the bias \n",
    "        self.input_bias = np.random.randn(1, 20)*0.0005\n",
    "        self.output_bias = np.random.randn(1, 1)*0.0005\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return ((np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)))\n",
    "    \n",
    "    def tanh_derivative(self, x):\n",
    "        return (1-(self.tanh(x))**2)\n",
    "\n",
    "    def train(self, X, Y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            X_length = len(X)\n",
    "            error = 0\n",
    "            # Pass training set through the neural network row by row\n",
    "            for x, y in zip(X, Y):\n",
    "                \n",
    "                x = np.array([x])\n",
    "               \n",
    "                # Layer 1\n",
    "                # x : input\n",
    "                # Ah : output\n",
    "                h = np.dot(x, self.input_hidden_weights) + self.input_bias\n",
    "                Ah = self.tanh(h)\n",
    "                \n",
    "                # Layer 2:\n",
    "                # Ah : input\n",
    "                # Yhat : final output\n",
    "                h2 = np.dot(Ah, self.output_hidden_weights) + self.output_bias\n",
    "                Yhat = self.sigmoid(h2)\n",
    "                \n",
    "                # Calculate the error rate (MSE DERIVATIVE)\n",
    "                # summ : wi*xi + bi\n",
    "                # error: d(loss)/d(output)\n",
    "                # activation error: d(loss)/d(o) * f'(summ)\n",
    "                # weights: d(loss)/d(w1): d(loss)/d(output) * d(out)/d(summ) * d(summ)/s(wi) : activation error * d(summ)/s(wi) : activation error * input\n",
    "                # bias: d(loss)/d(bias) : f'(x) * d(summ)/d(b) = f'(x)\n",
    "                # input: d(loss)/d(input) : d(loss)/d(summ) * d(summ)/d(in) = f'(x) * wi\n",
    "                error += np.mean(np.square(y-Yhat))\n",
    "                # derivatives \n",
    "                der_error = y-Yhat\n",
    "                # (der_error) * self.sigmoid_derivative(h2) is the activation error\n",
    "                act_error_1 = (der_error) * self.sigmoid_derivative(h2)\n",
    "                backprop_error = np.dot((der_error) * self.sigmoid_derivative(h2), self.output_hidden_weights.T)\n",
    "                act_error_2 = (backprop_error) * self.tanh_derivative(h)\n",
    "\n",
    "                # Differential of d(xi*wi + bi)/d(wi) = xi (input to any layer) (Here input to L2 is Ah)\n",
    "                grad_output_hidden_weights = np.dot(Ah.T, act_error_1)\n",
    "                # (Here input to L1 is x)\n",
    "                grad_input_hidden_weights = np.dot(x.T, act_error_2)\n",
    "                # Differential of d(xi*wi + bi)/d(bi) = 1\n",
    "                grad_output_bias = act_error_1\n",
    "                grad_input_bias = act_error_2\n",
    "\n",
    "\n",
    "                # updation of the weights and biases\n",
    "                self.output_hidden_weights += self.lr * grad_output_hidden_weights\n",
    "                self.input_hidden_weights += self.lr * grad_input_hidden_weights\n",
    "                self.output_bias += self.lr * grad_output_bias\n",
    "                self.input_bias += self.lr * grad_input_bias\n",
    "            if(not epoch%100):\n",
    "                print(epoch, error/X_length)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Pass inputs through the neural network to get output\n",
    "        \"\"\"\n",
    "        h = np.dot(X,self.input_hidden_weights) + self.input_bias\n",
    "        Ah = self.sigmoid(h)\n",
    "        h2 = np.dot(Ah,self.output_hidden_weights) + self.output_bias\n",
    "        Yhat = self.sigmoid(h2)\n",
    "        return Yhat\n",
    "    \n",
    "#NOT SURE IF WE SHOULD ADD MORE LAYERS, PLS CHECK IT OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 9) (96, 1)\n",
      "Starting Training\n",
      "0 0.21820887502282959739\n",
      "100 0.16945625843968560988\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "accuracy : 0.85\n",
      "Confusion Matrix : \n",
      "[[1, 3], [0, 16]]\n",
      "\n",
      "\n",
      "Precision : 0.8421052631578947\n",
      "Recall : 1.0\n",
      "F1 SCORE : 0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize the single neuron neural network\n",
    "    neural_network = NeuralNetwork()\n",
    "    X = np.array(features, dtype=np.float128)\n",
    "    Y = np.array(labels, dtype=np.float128)    \n",
    "    print(X.shape, Y.shape)\n",
    "    print(\"Starting Training\")\n",
    "    # split into train test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "    # Train the neural network\n",
    "    neural_network.train(X_train, Y_train, 200)\n",
    "    Yhat = neural_network.predict(X_test)\n",
    "    Y_hat = [1 if i>0.6 else 0 for i in Yhat]\n",
    "    print(Y_test, Y_hat)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i,j in zip(Y_test,Y_hat):\n",
    "        if i==1 and j==1:\n",
    "            TP+=1\n",
    "        elif i==0 and j==0:\n",
    "            TN+=1\n",
    "        elif i==1 and j==0:\n",
    "            FN+=1\n",
    "        elif i==0 and j==1:\n",
    "            FP+=1\n",
    "            \n",
    "    accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "    print(f\"accuracy : {accuracy}\")\n",
    "    cm=[[0,0],[0,0]]\n",
    "    cm[0][0]=TN\n",
    "    cm[0][1]=FP\n",
    "    cm[1][0]=FN\n",
    "    cm[1][1]=TP\n",
    "    # May need to add try except to avoid divide by 0\n",
    "    p= TP/(TP+FP)\n",
    "    r= TP/(TP+FN)\n",
    "    f1=(2*p*r)/(p+r)\n",
    "    print(\"Confusion Matrix : \")\n",
    "    print(cm)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Precision : {p}\")\n",
    "    print(f\"Recall : {r}\")\n",
    "    print(f\"F1 SCORE : {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
